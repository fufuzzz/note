#### 保存数据的流程 

1、在爬虫文件的parse方法中， 只用来获取数据，不要做保存数据的操作，因为这样用不到多线程所以要通过yield把封装的数据交给pipeline

2、在配置文件中，把pipeline的配置打开

```python
ITEM_PIPELINES = {
   'work2_scrapy_douban.pipelines.Work2ScrapyDoubanPipeline': 300,
    
    # 如果有多个pipeline路径，那么这些pippeline都会去处理yield返回的数据
    # 值代表的是权重， 值越小，越先执行
   'work2_scrapy_douban.pipelines.Work2ScrapyDoubanPipeline2': 301,
}

```

3、在pipeline文件中，定义如下方法

```python
class Work2ScrapyDoubanPipeline:
    # open_spider 在爬虫运行开始的时候先执行,相当于__init__方法
    # 参数spider的作用是,区分是哪个爬虫
    
    # 比如现在有两个爬虫
    '''
    spider
    	__init__.py
    	douban.py # 爬虫
    	dy.py  # 爬虫
    
    
    '''
    # def __init__(self):
    #     self.fp = open('data.json', 'a', encoding='utf-8')

    def open_spider(self, spider):
        if spider.name == 'douban':
        	self.fp = open('douban.json', 'a', encoding='utf-8')
		else:
            self.fp = open('dy.json', 'a', encoding='utf-8')
    def process_item(self, item, spider):
        print(item)
        # if os.path.exists('./douban'):
        #     os.mkdir('./douban/')
        # with open('douban.json', 'a', encoding='utf-8') as fp:
        json.dump(item, self.fp, ensure_ascii=False)
        self.fp.write(',')

        return item

    def close_spider(self, spider):
        print(spider.name)
        print('爬虫结束')
        self.fp.close()

    # def __del__(self):
    #     self.fp.close()
# 一般是一个项目一个爬虫 
```

#### JsonLinesItemExporter方法

如果要把数据通过json形式保存在文件中的话, 那么pipeline文件中,应该使用JsonLinesItemExporter方法

```python
class Work2ScrapyDoubanPipeline:
    # def __init__(self):
    #     self.fp = open('data.json', 'a', encoding='utf-8')

    def open_spider(self, spider):
        self.fp = open('douban3.json', 'wb')
        self.exporter = JsonLinesItemExporter(self.fp, ensure_ascii=False, encoding='utf-8')

    def process_item(self, item, spider):
        self.exporter.export_item(item)

        return item

    def close_spider(self, spider):
        self.fp.close()
```



#### 在爬虫文件中, 请求其他的链接地址

- 在parse方法中, 通过yield scrapy.Request方法中请求地址, Request方法 有两个必填的参数

  参数:

  - url:  请求的网址
  - callback: 回调函数, 当爬虫获取请求的源代码之后, 会交给callback定义的方法来处理数据

```python
def parse(self, response):
        # 获取meta从参数的方法
        page = response.meta.get('page', 1)
        print(page)

        # response.urljoin 在不完整的页面补充
        hrefs = [response.urljoin(href) for href in response.xpath('//*[@id="itemContainer"]/div/div/h3/a/@href').getall()]

        for href in hrefs:
            # 怎么用scrapy去访问每个链接地址
            yield scrapy.Request(url=href, callback=self.parse_detail)

        while True:
            if page == 3:
                break
            page += 1
            url = f''
            yield scrapy.Request(url=url, callback=self.parse_detail)



    def parse_detail(self, response):
            title = response.xpath('//*[@id="ct"]/div[1]/div/div[1]/div/div[2]/div[1]/h1/text()').getall()
            desc = response.xpath('//*[@id="ct"]/div[1]/div/div[1]/div/div[3]/p/text()').getall()
            item = WxappItem(title=title, desc=desc)
            yield item
```



#### 传递请求的参数

方法: scrapy.Request(url, callback, meta={'k':'v', 'k':'v'})  meta是传递的参数, 字典形式

#### 获取传递的参数

方法: response.meta.get('k') 通过响应对象的response的meta属性,格式是字典

如果项目初始的请求方式就是post的话, 那么需要在爬虫文件中重写start_requset方法 

```python
def start_requests(self):
        data = {
            'kw': 'world'
        }
        yield scrapy.FormRequest(url='https://fanyi.baidu.com/sug', formdata=data, callback=self.post_data)
```





#### scrapy中 中间件的使用, scrap有两种中间件, 一个是爬虫中间件(这个一般用不到), 一个是下载器中间件。中间件，用的比较多的方法是process_request,这个 方法是设置请求头中的一些参数,和返回响应的自定义的源代码,一般最常用的就是设置ip代理和随机请求头

- ip代理的设置方式

```python
class downloader1:
    proxy_list = [
        '119.132.76.144:4256',
        '182.135.158.137:4226',
        '42.242.122.222:4243'
    ]

    def process_request(self, request, spider):
        data = random.choice(self.proxy_list)
        request.meta['proxy'] = f'http://{data}'
        # return HtmlResponse(url=request.url, body=b'haha 123456', request=request)

    def process_response(self, request, response, spider):
        # print('response1')
        return response

```

- 随机请求头的方式

```python
class downloader2:
    USER_AGENTS = [

        "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36",

        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36",

        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36",

        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36",

        "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2226.0 Safari/537.36",

    ]

    def process_request(self, request, spider):
        data = random.choice(self.USER_AGENTS)
        request.headers['user-agent'] = data

    def process_response(self, request, response, spider):
        return response
```

注意: 一定要在配置文件中打开中间件的配置

```python
DOWNLOADER_MIDDLEWARES = {
   'httpbin.middlewares.downloader1': 543,
   'httpbin.middlewares.downloader2': 544,
}
```



##### 了解: 如果有多个中间件,那么中间件中方法的执行顺序

- process_request 返回None

  process_request的方法是配置文件中中间件定义顺序的从上往下执行

  process_response的方法是从下往上执行

- process_request返回响应对象

  不会在执行下一个中间件的process_request方法,

  process_response方法还是从下往上执行

  爬虫中获取的源代码是响应对象中设置的代码,比如

  ```python
  def process_request(self, request, spider)
  	return HtmlResponse(url=request.url, body=b'返回的内容', request=request)
  ```

  

#### 在scrapy中使用selenium

在中间件设置

```python
class SeleniumMiddware:
    def __init__(self):
        chrome_path = r'D:\chromedriver\chromedriver.exe'
        self.driver = webdriver.Chrome(executable_path=chrome_path)
    
    def process_request(self, request, spider):
        self.driver.get(request.url)
        data = self.driver.page_soure
        
        return HtmlResponse(url=request.url, body=data, request=request, encoding='utf-8')
```

注意事项:一定要在配置文件中打开中间件配置

```python
DOWNLOADER_MIDDLEWARES = {
   # 'jianshu.middlewares.JianshuDownloaderMiddleware': 543,
   'jianshu.middlewares.SeleniumMiddware': 543,
}
```

