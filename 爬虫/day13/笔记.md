#### 规则爬虫的流程

- 创建规则爬虫
  - 创建项目
  - 创建规则爬虫：`scrapy genspider -t crawl 爬虫名称 域名`

- 最重要的就是创建 正则去 匹配 要请求的网址

  ```python
  class AppSpider(CrawlSpider):
      name = 'app'
      allowed_domains = ['dreawer.com']
      start_urls = ['http://wxapp.dreawer.com/portal.php?mod=list&catid=2&page=1']
  	
      
      # 重要
      rules = (
          # LinkExtractor(allow=r'') 在获取的初始页面的源代码中 去 匹配 allow参数中 链接地址的正则，如果能匹配到，就去请求这个页面
          # callback 把请求到的页面源码交给 callback 方法去 获取特定数据
          # follow 表示跟随，如果为 True，表示请求其他页面地址的时候，继续去匹配路由正则
          # 如果为 Fasle，表示当请求其它页面，不再去 匹配路由正则
          Rule(LinkExtractor(allow=r'.+mod=list&catid=2&page=\d+'), follow=True),
          Rule(LinkExtractor(allow=r'.+article-\d+-\d+\.html'), callback='parse_item', follow=False),
      )
  ```





#### sscrapy-redis 项目的搭建，比如以  爬取 `小程序社区` 为例

- 创建一个普通的规则爬虫

  - 创建项目
  - 创建爬虫 scrapy genspider -t crawl 爬虫名称 域名

- 在 爬虫程序里面去写 爬虫规则

  ```python
  import scrapy
  from scrapy.linkextractors import LinkExtractor
  from scrapy.spiders import CrawlSpider, Rule
  
  
  class AppSpider(CrawlSpider):
      name = 'app'
      allowed_domains = ['dreawer.com']
      start_urls = ['http://wxapp.dreawer.com/portal.php?mod=list&catid=2&page=1']
  
      rules = (
          # LinkExtractor(allow=r'') 在获取的初始页面的源代码中 去 匹配 allow参数中 链接地址的正则，如果能匹配到，就去请求这个页面
          # callback 把请求到的页面源码交给 callback 方法去 获取特定数据
          # follow 表示跟随，如果为 True，表示请求其他页面地址的时候，继续去匹配路由正则
          # 如果为 Fasle，表示当请求其它页面，不再去 匹配路由正则
          Rule(LinkExtractor(allow=r'.+mod=list&catid=2&page=\d+'), follow=True),
          Rule(LinkExtractor(allow=r'.+article-\d+-\d+\.html'), callback='parse_item', follow=False),
      )
  
      def parse_item(self, response):
          con = response.xpath('//*[@id="ct"]/div[1]/div/div[1]/div/div[2]/div[1]/h1/text()').get()
          print(con)
  ```

- 能够 让这个 规则爬虫 先 运行起来

- 修改 爬虫中 的一些设置，让爬虫能够使用 scrapy_redis 的 逻辑

  - 把 爬虫继承的 CrawlSpider父类  改成 RedisCrawlSpider

  - 去掉   start_urls

  - 在 爬虫类 添加  redis_key 属性，可以把这个属性 看成 redis 队列的 名称，它的值一般叫做  redis_key = 'start:url'

    整个爬虫类 应该是一下写法

    ```
    import scrapy
    from scrapy.linkextractors import LinkExtractor
    from scrapy.spiders import CrawlSpider, Rule
    from scrapy_redis.spiders import RedisCrawlSpider
    
    from ..items import AppredisItem
    
    
    class AppSpider(RedisCrawlSpider):
        name = 'app'
        allowed_domains = ['dreawer.com']
        # start_urls = ['http://dreawer.com/']
        redis_key = 'start:url'
    
        rules = (
            # LinkExtractor(allow=r'') 在获取的初始页面的源代码中 去 匹配 allow参数中 链接地址的正则，如果能匹配到，就去请求这个页面
            # callback 把请求到的页面源码交给 callback 方法去 获取特定数据
            # follow 表示跟随，如果为 True，表示请求其他页面地址的时候，继续去匹配路由正则
            # 如果为 Fasle，表示当请求其它页面，不再去 匹配路由正则
            Rule(LinkExtractor(allow=r'.+mod=list&catid=2&page=\d+'), follow=True),
            Rule(LinkExtractor(allow=r'.+article-\d+-\d+\.html'), callback='parse_item', follow=False),
        )
    
        def parse_item(self, response):
            title = response.xpath('//*[@id="ct"]/div[1]/div/div[1]/div/div[2]/div[1]/h1/text()').get()
            desc = response.xpath('//*[@id="ct"]/div[1]/div/div[1]/div/div[3]/p/text()').get()
    
            item = AppredisItem(
                title=title,
                desc=desc
            )
    
            yield item
    ```

- 修改 配置文件

  - 启用Redis调度存储请求队列： 添加配置是 `SCHEDULER = "scrapy_redis.scheduler.Scheduler"`

  - 确保所有的爬虫通过Redis去重：  添加配置是 `DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"`

  - 处理yield 返回的数据，是通过 scrapy_redis默认的pipeline进行，这个pipeline默认是把数据保存在 redis的队列中

    添加配置：

    ```python
    ITEM_PIPELINES = {
        'scrapy_redis.pipelines.RedisPipeline': 300
    }
    ```

  - 不清除Redis队列、这样可以暂停/恢复 爬取:  添加配置 `SCHEDULER_PERSIST = True`

  - 指定连接到redis时使用的端口和地址:

    添加配置

    ```python
    REDIS_HOST = '127.0.0.1'
    REDIS_PORT = 6379
    ```

- 开启 redis 服务器，  找到 redis 的安装目录， 双击 下面的 redis-server.exe 程序

- 如果有多个 服务器，那么就需要把 项目代码 放入到 多个服务器中，进入项目，执行爬虫. 注意 redis的ip应该是 存放 redis服务的 外网 ip， 状态是阻塞的，因为没有获取到第一个 url
- 打开 redis 的客户端， 找到 redis 的安装目录，点击下面的 redis-cli.exe 的程序
- 添加一个 redis的 队列，名字叫做 start:url  ,  值就是 第一个要 爬起的 页面， 命令就是  `lpush start:url http://wxapp.dreawer.com/portal.php?mod=list&catid=2`

- 添加完成之后， 所有的爬虫 由 阻塞状态 就会变成 爬取数据的 状态
- 爬取数据后，在 redis 里面会创建 3个 变量
  - dupefilter:  保存去重的网址
  - items:  保存的是爬取的数据
  - requests： 即将要 爬取的网址